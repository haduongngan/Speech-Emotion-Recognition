{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\nfrom datetime import datetime\nimport pickle\n\nimport librosa\nimport librosa.display\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch import nn\nfrom torch import optim as opt\nfrom torch.utils.data import Dataset, DataLoader \nimport torchvision\nfrom torchvision import transforms as T, datasets  \nfrom torch.utils.tensorboard import SummaryWriter \n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:31:56.081558Z","iopub.execute_input":"2022-06-21T17:31:56.081937Z","iopub.status.idle":"2022-06-21T17:32:00.891529Z","shell.execute_reply.started":"2022-06-21T17:31:56.081870Z","shell.execute_reply":"2022-06-21T17:32:00.890658Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"Ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/\"\nTess = \"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/\"","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:32:38.306941Z","iopub.execute_input":"2022-06-21T17:32:38.307324Z","iopub.status.idle":"2022-06-21T17:32:38.311688Z","shell.execute_reply.started":"2022-06-21T17:32:38.307293Z","shell.execute_reply":"2022-06-21T17:32:38.310863Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"ravdess_directory_list = os.listdir(Ravdess)\nravdess_emotion = ['neutral','calm','happy','sad','angry','fear','disgust','surprise']\n\nfile_emotion = []\nfile_path = []\nfor dir in ravdess_directory_list:\n    if (dir == \"audio_speech_actors_01-24\"):\n        continue\n    actor = os.listdir(Ravdess + dir)\n    for file in actor:\n        # get the emotion of this file\n        part = file.split('.')[0]\n        part = part.split('-')\n        id = int(part[2])\n        file_emotion.append(ravdess_emotion[id-1])\n        # get file's path\n        file_path.append(Ravdess + dir + '/' + file)\n        \n# convert to dataframe\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\npath_df = pd.DataFrame(file_path, columns=['Path'])\nRavdess_df = pd.concat([emotion_df, path_df], axis=1)\n\nRavdess_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:32:39.434899Z","iopub.execute_input":"2022-06-21T17:32:39.435275Z","iopub.status.idle":"2022-06-21T17:32:39.668684Z","shell.execute_reply.started":"2022-06-21T17:32:39.435243Z","shell.execute_reply":"2022-06-21T17:32:39.667755Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(1440, 2)"},"metadata":{}}]},{"cell_type":"code","source":"tess_directory_list = os.listdir(Tess)\n\nfile_emotion = []\nfile_path = []\n\nfor dir in tess_directory_list:\n    directories = os.listdir(Tess + dir)\n    for file in directories:\n        part = file.split('.')[0]\n        part = part.split('_')[2]\n        if part=='ps':\n            file_emotion.append('surprise')\n        else:\n            file_emotion.append(part)\n        file_path.append(Tess + dir + '/' + file)\n        \n# convert to dataframe\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\npath_df = pd.DataFrame(file_path, columns=['Path'])\nTess_df = pd.concat([emotion_df, path_df], axis=1)\n\nTess_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:32:41.727696Z","iopub.execute_input":"2022-06-21T17:32:41.728078Z","iopub.status.idle":"2022-06-21T17:32:42.077672Z","shell.execute_reply.started":"2022-06-21T17:32:41.728032Z","shell.execute_reply":"2022-06-21T17:32:42.076751Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(2800, 2)"},"metadata":{}}]},{"cell_type":"code","source":"data_path = pd.concat([Ravdess_df, Tess_df], axis = 0)\ndata_path.to_csv(\"data_path.csv\",index=False)\ndata_path.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:32:44.605048Z","iopub.execute_input":"2022-06-21T17:32:44.605443Z","iopub.status.idle":"2022-06-21T17:32:44.644912Z","shell.execute_reply.started":"2022-06-21T17:32:44.605412Z","shell.execute_reply":"2022-06-21T17:32:44.643988Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   Emotions                                               Path\n0  surprise  /kaggle/input/ravdess-emotional-speech-audio/A...\n1   neutral  /kaggle/input/ravdess-emotional-speech-audio/A...\n2   disgust  /kaggle/input/ravdess-emotional-speech-audio/A...\n3   disgust  /kaggle/input/ravdess-emotional-speech-audio/A...\n4   neutral  /kaggle/input/ravdess-emotional-speech-audio/A...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotions</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>surprise</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>disgust</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>neutral</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def extract_features(data,sample_rate):\n    # ZCR\n    result = np.array([])\n    zcr = np.mean(librosa.feature.zero_crossing_rate(y = data).T, axis = 0)\n    result=np.hstack((result, zcr))\n\n    # MFCC\n    mfcc = np.mean(librosa.feature.mfcc(y = data, sr = sample_rate).T, axis = 0)\n    result = np.hstack((result, mfcc))\n    \n    # Log Mel-Spectrogram\n    mel = np.mean(librosa.feature.melspectrogram(y = data, sr = sample_rate).T, axis = 0)\n    result = np.hstack((result, mel)) \n    \n    # Chroma\n    chroma_stft = np.mean(librosa.feature.chroma_stft(S = np.abs(librosa.stft(data)), sr = sample_rate).T, axis = 0)\n    result = np.hstack((result, chroma_stft))\n\n    # Root Mean Square Value\n    rms = np.mean(librosa.feature.rms(y = data).T, axis = 0)\n    result = np.hstack((result, rms))\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2022-06-21T12:50:28.558600Z","iopub.execute_input":"2022-06-21T12:50:28.558955Z","iopub.status.idle":"2022-06-21T12:50:28.568335Z","shell.execute_reply.started":"2022-06-21T12:50:28.558925Z","shell.execute_reply":"2022-06-21T12:50:28.567684Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def noise(data):\n    noise_amp = 0.03 * np.random.uniform() * np.amax(data)\n    data = data + noise_amp * np.random.normal(size = data.shape[0])\n    return data\n\ndef stretch(data, rate = 0.8):\n    return librosa.effects.time_stretch(data, rate)\n\ndef pitch(data, sampling_rate, pitch_factor = 0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T12:50:33.269291Z","iopub.execute_input":"2022-06-21T12:50:33.269795Z","iopub.status.idle":"2022-06-21T12:50:33.277625Z","shell.execute_reply.started":"2022-06-21T12:50:33.269755Z","shell.execute_reply":"2022-06-21T12:50:33.277057Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X, Y = [], []\nfor path, emotion in zip(data_path.Path, data_path.Emotions):\n    # load data\n    data, sample_rate = librosa.load(path, duration=3)\n    \n    # augmentation\n    noise_data = noise(data)\n    stretch_pitch_data = stretch(data)\n    stretch_pitch_data = pitch(stretch_pitch_data, sample_rate)\n    \n    # original speech\n    feature = extract_features(data, sample_rate)\n    feature = np.array(feature)\n    X.append(feature)\n    Y.append(emotion)\n    \n    # noise speech\n    feature_noise = extract_features(noise_data, sample_rate)\n    feature_noise = np.array(feature_noise)\n    X.append(feature_noise)\n    Y.append(emotion)\n    \n    # stretch and pitch speech\n    feature_stretch_pitch = extract_features(stretch_pitch_data, sample_rate)\n    feature_stretch_pitch = np.array(feature_stretch_pitch)\n    X.append(feature_stretch_pitch)\n    Y.append(emotion)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:30:56.287724Z","iopub.execute_input":"2022-06-17T16:30:56.288065Z","iopub.status.idle":"2022-06-17T16:58:50.242113Z","shell.execute_reply.started":"2022-06-17T16:30:56.288034Z","shell.execute_reply":"2022-06-17T16:58:50.240602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert to df and save\nFeatures = pd.DataFrame(X)\nFeatures['labels'] = Y\nFeatures.to_csv('features.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T17:03:04.152955Z","iopub.execute_input":"2022-06-17T17:03:04.15333Z","iopub.status.idle":"2022-06-17T17:03:11.004115Z","shell.execute_reply.started":"2022-06-17T17:03:04.153298Z","shell.execute_reply":"2022-06-17T17:03:11.003251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load features from file\nFeatures = pd.read_csv(\"./features.csv\")\n# Features = pd.read_csv(\"../input/feature1/features.csv\")\nX = Features.iloc[: ,:-1].values\nY = Features['labels'].values\nlen(X), len(Y), data_path.Path.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:33:04.489448Z","iopub.execute_input":"2022-06-21T17:33:04.489817Z","iopub.status.idle":"2022-06-21T17:33:05.094950Z","shell.execute_reply.started":"2022-06-21T17:33:04.489786Z","shell.execute_reply":"2022-06-21T17:33:05.094153Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(12720, 12720, (4240,))"},"metadata":{}}]},{"cell_type":"code","source":"# As this is a multiclass classification problem onehotencoding our Y.\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:33:06.786417Z","iopub.execute_input":"2022-06-21T17:33:06.786776Z","iopub.status.idle":"2022-06-21T17:33:06.796738Z","shell.execute_reply.started":"2022-06-21T17:33:06.786739Z","shell.execute_reply":"2022-06-21T17:33:06.795969Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# splitting data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:33:08.404433Z","iopub.execute_input":"2022-06-21T17:33:08.405035Z","iopub.status.idle":"2022-06-21T17:33:08.446617Z","shell.execute_reply.started":"2022-06-21T17:33:08.405002Z","shell.execute_reply":"2022-06-21T17:33:08.445891Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"((9540, 162), (9540, 8), (3180, 162), (3180, 8))"},"metadata":{}}]},{"cell_type":"code","source":"import pickle\n\n# scaling our data and save the scaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\n\npickle.dump(scaler, open('scaler_two.pkl','wb'))\nscaler = pickle.load(open('scaler_two.pkl','rb'))\n\nx_test = scaler.transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:33:12.903480Z","iopub.execute_input":"2022-06-21T17:33:12.904132Z","iopub.status.idle":"2022-06-21T17:33:12.932666Z","shell.execute_reply.started":"2022-06-21T17:33:12.904092Z","shell.execute_reply":"2022-06-21T17:33:12.931911Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"((9540, 162), (9540, 8), (3180, 162), (3180, 8))"},"metadata":{}}]},{"cell_type":"code","source":"class Dataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        X = torch.tensor(self.X[idx]).type(torch.float)\n        y = torch.tensor(self.y[idx]).type(torch.float)\n        # y = self.y[idx]\n\n        return X, y","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:33:16.497818Z","iopub.execute_input":"2022-06-21T17:33:16.498192Z","iopub.status.idle":"2022-06-21T17:33:16.504499Z","shell.execute_reply.started":"2022-06-21T17:33:16.498159Z","shell.execute_reply":"2022-06-21T17:33:16.503445Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#Convert X to tensor\nX_train_2 = torch.from_numpy(x_train)\nX_test_2 = torch.from_numpy(x_test)\nprint(X_train_2.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:33:18.714173Z","iopub.execute_input":"2022-06-21T17:33:18.714603Z","iopub.status.idle":"2022-06-21T17:33:18.723303Z","shell.execute_reply.started":"2022-06-21T17:33:18.714564Z","shell.execute_reply":"2022-06-21T17:33:18.722391Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"torch.Size([9540, 162])\n","output_type":"stream"}]},{"cell_type":"code","source":"BATCH_SIZE = 64\ntrain_data = Dataset(X_train_2, y_train)\ntest_data = Dataset(X_test_2, y_test)\n\ntrain_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=os.cpu_count(), shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, num_workers=os.cpu_count())","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:33:20.249565Z","iopub.execute_input":"2022-06-21T17:33:20.249981Z","iopub.status.idle":"2022-06-21T17:33:20.256958Z","shell.execute_reply.started":"2022-06-21T17:33:20.249941Z","shell.execute_reply":"2022-06-21T17:33:20.256278Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n    \n    def __init__(self, ):\n        super(CNN, self).__init__()\n        \n        # Block #1: \n        self.layer1 = nn.Sequential(\n            nn.Conv1d(in_channels=1, out_channels=256, kernel_size=5),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=5, stride=2)\n        )\n        \n        # Block #2:  \n        self.layer2 = nn.Sequential(\n            nn.Conv1d(in_channels=256, out_channels=256, kernel_size=5),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=5, stride=2)\n        )\n\n        # Block #3: \n        self.layer3 = nn.Sequential(\n            nn.Conv1d(in_channels=256, out_channels=128, kernel_size=5),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=5, stride=2)\n        )\n        \n        # Block #4: \n        self.layer4 = nn.Sequential(\n            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=5),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=5, stride=2)\n        )\n        \n        # Block #5: \n        self.layer5 = nn.Sequential(\n            nn.Linear(in_features=192, out_features=32),\n            nn.ReLU(),\n            nn.Dropout(p=0.3)\n        )\n\n        # FC 5 → softmax\n        self.fc = nn.Linear(in_features=32, out_features=8)\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        \n        # Channel x H = 1 x 162 \n        out = self.layer1(x.view(-1, 1, 162))\n        \n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        \n        out = out.view(out.size(0), -1) \n        out = self.layer5(out)\n        out = self.fc(out)\n        out = self.softmax(out)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:33:22.440615Z","iopub.execute_input":"2022-06-21T17:33:22.441107Z","iopub.status.idle":"2022-06-21T17:33:22.457220Z","shell.execute_reply.started":"2022-06-21T17:33:22.441049Z","shell.execute_reply":"2022-06-21T17:33:22.456000Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# model = CNN()\n# print(model)\n!pip install torch-summary\nfrom torchsummary import summary\n\nmodel = CNN()\nsummary(model, (64, 162))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:33:25.674306Z","iopub.execute_input":"2022-06-21T17:33:25.674778Z","iopub.status.idle":"2022-06-21T17:33:48.494758Z","shell.execute_reply.started":"2022-06-21T17:33:25.674734Z","shell.execute_reply":"2022-06-21T17:33:48.493831Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Collecting torch-summary\n  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\nInstalling collected packages: torch-summary\nSuccessfully installed torch-summary-1.4.5\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\n├─Sequential: 1-1                        [-1, 256, 77]             --\n|    └─Conv1d: 2-1                       [-1, 256, 158]            1,536\n|    └─ReLU: 2-2                         [-1, 256, 158]            --\n|    └─MaxPool1d: 2-3                    [-1, 256, 77]             --\n├─Sequential: 1-2                        [-1, 256, 35]             --\n|    └─Conv1d: 2-4                       [-1, 256, 73]             327,936\n|    └─ReLU: 2-5                         [-1, 256, 73]             --\n|    └─MaxPool1d: 2-6                    [-1, 256, 35]             --\n├─Sequential: 1-3                        [-1, 128, 14]             --\n|    └─Conv1d: 2-7                       [-1, 128, 31]             163,968\n|    └─ReLU: 2-8                         [-1, 128, 31]             --\n|    └─MaxPool1d: 2-9                    [-1, 128, 14]             --\n├─Sequential: 1-4                        [-1, 64, 3]               --\n|    └─Conv1d: 2-10                      [-1, 64, 10]              41,024\n|    └─ReLU: 2-11                        [-1, 64, 10]              --\n|    └─MaxPool1d: 2-12                   [-1, 64, 3]               --\n├─Sequential: 1-5                        [-1, 32]                  --\n|    └─Linear: 2-13                      [-1, 32]                  6,176\n|    └─ReLU: 2-14                        [-1, 32]                  --\n|    └─Dropout: 2-15                     [-1, 32]                  --\n├─Linear: 1-6                            [-1, 8]                   264\n├─Softmax: 1-7                           [-1, 8]                   --\n==========================================================================================\nTotal params: 540,904\nTrainable params: 540,904\nNon-trainable params: 0\nTotal mult-adds (M): 30.16\n==========================================================================================\nInput size (MB): 0.04\nForward/backward pass size (MB): 0.49\nParams size (MB): 2.06\nEstimated Total Size (MB): 2.59\n==========================================================================================\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\n├─Sequential: 1-1                        [-1, 256, 77]             --\n|    └─Conv1d: 2-1                       [-1, 256, 158]            1,536\n|    └─ReLU: 2-2                         [-1, 256, 158]            --\n|    └─MaxPool1d: 2-3                    [-1, 256, 77]             --\n├─Sequential: 1-2                        [-1, 256, 35]             --\n|    └─Conv1d: 2-4                       [-1, 256, 73]             327,936\n|    └─ReLU: 2-5                         [-1, 256, 73]             --\n|    └─MaxPool1d: 2-6                    [-1, 256, 35]             --\n├─Sequential: 1-3                        [-1, 128, 14]             --\n|    └─Conv1d: 2-7                       [-1, 128, 31]             163,968\n|    └─ReLU: 2-8                         [-1, 128, 31]             --\n|    └─MaxPool1d: 2-9                    [-1, 128, 14]             --\n├─Sequential: 1-4                        [-1, 64, 3]               --\n|    └─Conv1d: 2-10                      [-1, 64, 10]              41,024\n|    └─ReLU: 2-11                        [-1, 64, 10]              --\n|    └─MaxPool1d: 2-12                   [-1, 64, 3]               --\n├─Sequential: 1-5                        [-1, 32]                  --\n|    └─Linear: 2-13                      [-1, 32]                  6,176\n|    └─ReLU: 2-14                        [-1, 32]                  --\n|    └─Dropout: 2-15                     [-1, 32]                  --\n├─Linear: 1-6                            [-1, 8]                   264\n├─Softmax: 1-7                           [-1, 8]                   --\n==========================================================================================\nTotal params: 540,904\nTrainable params: 540,904\nNon-trainable params: 0\nTotal mult-adds (M): 30.16\n==========================================================================================\nInput size (MB): 0.04\nForward/backward pass size (MB): 0.49\nParams size (MB): 2.06\nEstimated Total Size (MB): 2.59\n=========================================================================================="},"metadata":{}}]},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, train_dataloader, test_dataloader,\n                 model, loss_fn, optimizer, logger, device='cpu'):\n        self.model = model.to(device)\n        self.train_dataloader = train_dataloader\n        self.test_dataloader = test_dataloader\n        self.logger = logger\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.device = device\n\n    def train_epoch(self):\n        # Train data\n        n_samples = len(self.train_dataloader.dataset)\n        train_loss = 0\n\n        for batch_idx, (X, y) in enumerate(self.train_dataloader):\n            X = X.to(self.device)\n            y = y.to(self.device)\n            # Forward\n            pred = self.model(X)\n            loss = self.loss_fn(pred, torch.argmax(y, dim=1))\n            \n            # Backward\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            train_loss += loss\n\n        return train_loss / n_samples\n\n    def test_epoch(self):\n        # Test data\n        n_samples = len(self.test_dataloader.dataset)\n        test_loss = 0\n\n        for batch_idx, (X,y) in enumerate(self.test_dataloader):\n            X = X.to(self.device)\n            y = y.to(self.device)\n            with torch.no_grad():\n                # Forward\n                pred = self.model(X)\n                loss = self.loss_fn(pred, torch.argmax(y, dim=1))\n\n            test_loss += loss\n\n        return test_loss / n_samples\n\n    def evaluation(self, dataloader):\n        y_true = []\n        y_pred = []\n        for X, y in dataloader:\n            X = X.to(self.device)\n            y_true.append(y.detach().cpu())\n            y_pred.append(self.model(X).detach().cpu())\n        \n        y_true = torch.cat(y_true, dim=0)\n        y_pred = torch.cat(y_pred, dim=0)\n\n        true_labels = torch.argmax(y_true, dim=1)\n        pred_labels = torch.argmax(y_pred, dim=1)\n        accuracy = accuracy_score(true_labels.cpu(), pred_labels.cpu())\n\n        return accuracy\n\n    def train(self, epochs=10):\n        for i in range(epochs):\n            self.current_epoch = i+1\n            # Training\n            train_loss = self.train_epoch()\n            test_loss = self.test_epoch()\n\n            # Evaluation\n            train_acc = self.evaluation(self.train_dataloader)\n            test_acc = self.evaluation(self.test_dataloader)\n            \n            # Logging\n            self.logger.add_scalar('Loss/train', train_loss.item(), i+1)\n            self.logger.add_scalar('Loss/test', test_loss.item(), i+1)\n            self.logger.add_scalar('Accuracy/train', train_acc.item(), i+1)\n            self.logger.add_scalar('Accuracy/test', test_acc.item(), i+1)\n\n            ## Log histogram\n            for name, params in model.named_parameters():\n                if 'weight' in name:\n                    self.logger.add_histogram(name, params, i+1)\n\n            # if ((i+1) % 10 == 0):\n            print(f\"Epoch {i+1}: Train Loss = {train_loss.item():.5f}, Test Loss = {test_loss.item():.5f}, \"\n            f\"Train accuracy score = {train_acc.item():.5f}, Test accuracy score = {test_acc.item():.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:33:51.523463Z","iopub.execute_input":"2022-06-21T17:33:51.523962Z","iopub.status.idle":"2022-06-21T17:33:51.594955Z","shell.execute_reply.started":"2022-06-21T17:33:51.523911Z","shell.execute_reply":"2022-06-21T17:33:51.594130Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nLEARNING_RATE = 1e-3\nLOG_DIR = \"./logs/\"\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \nprint(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:33:55.596637Z","iopub.execute_input":"2022-06-21T17:33:55.596986Z","iopub.status.idle":"2022-06-21T17:33:55.602727Z","shell.execute_reply.started":"2022-06-21T17:33:55.596956Z","shell.execute_reply":"2022-06-21T17:33:55.602010Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"model = CNN()\nloss_fn = nn.CrossEntropyLoss() \noptimizer = opt.Adam(model.parameters(), lr=LEARNING_RATE)\nlogger = SummaryWriter(os.path.join(LOG_DIR, datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\"))) # Logger\n\n# Trainer\ntrainer = Trainer(\n    train_dataloader=train_dataloader,\n    test_dataloader=test_dataloader,\n    model=model,\n    loss_fn=loss_fn,\n    optimizer=optimizer,\n    logger=logger,\n    device=DEVICE\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:33:58.207586Z","iopub.execute_input":"2022-06-21T17:33:58.207944Z","iopub.status.idle":"2022-06-21T17:34:04.216186Z","shell.execute_reply.started":"2022-06-21T17:33:58.207914Z","shell.execute_reply":"2022-06-21T17:34:04.215291Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\ntrainer.train(epochs=150)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:34:08.906429Z","iopub.execute_input":"2022-06-21T17:34:08.906794Z","iopub.status.idle":"2022-06-21T17:41:27.251095Z","shell.execute_reply.started":"2022-06-21T17:34:08.906763Z","shell.execute_reply":"2022-06-21T17:41:27.248664Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch 1: Train Loss = 0.03147, Test Loss = 0.03089, Train accuracy score = 0.29119, Test accuracy score = 0.29245\nEpoch 2: Train Loss = 0.03037, Test Loss = 0.02974, Train accuracy score = 0.37778, Test accuracy score = 0.38553\nEpoch 3: Train Loss = 0.02911, Test Loss = 0.02873, Train accuracy score = 0.45042, Test accuracy score = 0.44371\nEpoch 4: Train Loss = 0.02816, Test Loss = 0.02828, Train accuracy score = 0.46593, Test accuracy score = 0.46855\nEpoch 5: Train Loss = 0.02774, Test Loss = 0.02708, Train accuracy score = 0.55524, Test accuracy score = 0.55692\nEpoch 6: Train Loss = 0.02719, Test Loss = 0.02711, Train accuracy score = 0.55325, Test accuracy score = 0.55314\nEpoch 7: Train Loss = 0.02684, Test Loss = 0.02703, Train accuracy score = 0.55252, Test accuracy score = 0.55377\nEpoch 8: Train Loss = 0.02664, Test Loss = 0.02643, Train accuracy score = 0.60042, Test accuracy score = 0.59057\nEpoch 9: Train Loss = 0.02635, Test Loss = 0.02653, Train accuracy score = 0.58690, Test accuracy score = 0.58082\nEpoch 10: Train Loss = 0.02623, Test Loss = 0.02629, Train accuracy score = 0.60199, Test accuracy score = 0.60220\nEpoch 11: Train Loss = 0.02600, Test Loss = 0.02593, Train accuracy score = 0.62442, Test accuracy score = 0.62296\nEpoch 12: Train Loss = 0.02602, Test Loss = 0.02598, Train accuracy score = 0.62568, Test accuracy score = 0.62107\nEpoch 13: Train Loss = 0.02595, Test Loss = 0.02610, Train accuracy score = 0.60807, Test accuracy score = 0.60189\nEpoch 14: Train Loss = 0.02590, Test Loss = 0.02601, Train accuracy score = 0.61342, Test accuracy score = 0.61824\nEpoch 15: Train Loss = 0.02589, Test Loss = 0.02562, Train accuracy score = 0.64266, Test accuracy score = 0.63931\nEpoch 16: Train Loss = 0.02551, Test Loss = 0.02548, Train accuracy score = 0.66090, Test accuracy score = 0.64969\nEpoch 17: Train Loss = 0.02522, Test Loss = 0.02512, Train accuracy score = 0.67977, Test accuracy score = 0.66226\nEpoch 18: Train Loss = 0.02520, Test Loss = 0.02520, Train accuracy score = 0.68208, Test accuracy score = 0.66572\nEpoch 19: Train Loss = 0.02487, Test Loss = 0.02487, Train accuracy score = 0.70545, Test accuracy score = 0.69686\nEpoch 20: Train Loss = 0.02486, Test Loss = 0.02515, Train accuracy score = 0.68124, Test accuracy score = 0.67327\nEpoch 21: Train Loss = 0.02454, Test Loss = 0.02427, Train accuracy score = 0.73501, Test accuracy score = 0.72327\nEpoch 22: Train Loss = 0.02416, Test Loss = 0.02438, Train accuracy score = 0.74057, Test accuracy score = 0.71289\nEpoch 23: Train Loss = 0.02408, Test Loss = 0.02444, Train accuracy score = 0.73795, Test accuracy score = 0.72075\nEpoch 24: Train Loss = 0.02411, Test Loss = 0.02420, Train accuracy score = 0.75199, Test accuracy score = 0.73396\nEpoch 25: Train Loss = 0.02403, Test Loss = 0.02428, Train accuracy score = 0.75052, Test accuracy score = 0.72421\nEpoch 26: Train Loss = 0.02382, Test Loss = 0.02413, Train accuracy score = 0.75996, Test accuracy score = 0.73553\nEpoch 27: Train Loss = 0.02371, Test Loss = 0.02408, Train accuracy score = 0.76300, Test accuracy score = 0.74371\nEpoch 28: Train Loss = 0.02370, Test Loss = 0.02398, Train accuracy score = 0.77851, Test accuracy score = 0.74560\nEpoch 29: Train Loss = 0.02379, Test Loss = 0.02394, Train accuracy score = 0.77872, Test accuracy score = 0.74843\nEpoch 30: Train Loss = 0.02366, Test Loss = 0.02402, Train accuracy score = 0.77243, Test accuracy score = 0.74277\nEpoch 31: Train Loss = 0.02339, Test Loss = 0.02378, Train accuracy score = 0.78973, Test accuracy score = 0.75849\nEpoch 32: Train Loss = 0.02339, Test Loss = 0.02380, Train accuracy score = 0.79969, Test accuracy score = 0.76069\nEpoch 33: Train Loss = 0.02327, Test Loss = 0.02367, Train accuracy score = 0.80639, Test accuracy score = 0.76604\nEpoch 34: Train Loss = 0.02326, Test Loss = 0.02382, Train accuracy score = 0.78491, Test accuracy score = 0.76006\nEpoch 35: Train Loss = 0.02326, Test Loss = 0.02366, Train accuracy score = 0.79948, Test accuracy score = 0.76855\nEpoch 36: Train Loss = 0.02323, Test Loss = 0.02375, Train accuracy score = 0.80377, Test accuracy score = 0.76258\nEpoch 37: Train Loss = 0.02377, Test Loss = 0.02388, Train accuracy score = 0.78407, Test accuracy score = 0.75220\nEpoch 38: Train Loss = 0.02350, Test Loss = 0.02382, Train accuracy score = 0.79392, Test accuracy score = 0.76038\nEpoch 39: Train Loss = 0.02310, Test Loss = 0.02369, Train accuracy score = 0.80000, Test accuracy score = 0.76572\nEpoch 40: Train Loss = 0.02312, Test Loss = 0.02381, Train accuracy score = 0.80356, Test accuracy score = 0.76038\nEpoch 41: Train Loss = 0.02311, Test Loss = 0.02363, Train accuracy score = 0.80440, Test accuracy score = 0.77107\nEpoch 42: Train Loss = 0.02301, Test Loss = 0.02359, Train accuracy score = 0.80828, Test accuracy score = 0.77358\nEpoch 43: Train Loss = 0.02300, Test Loss = 0.02337, Train accuracy score = 0.82306, Test accuracy score = 0.78365\nEpoch 44: Train Loss = 0.02293, Test Loss = 0.02347, Train accuracy score = 0.82201, Test accuracy score = 0.78270\nEpoch 45: Train Loss = 0.02302, Test Loss = 0.02349, Train accuracy score = 0.81656, Test accuracy score = 0.77925\nEpoch 46: Train Loss = 0.02296, Test Loss = 0.02359, Train accuracy score = 0.81719, Test accuracy score = 0.77421\nEpoch 47: Train Loss = 0.02286, Test Loss = 0.02364, Train accuracy score = 0.81226, Test accuracy score = 0.76164\nEpoch 48: Train Loss = 0.02291, Test Loss = 0.02344, Train accuracy score = 0.82400, Test accuracy score = 0.78522\nEpoch 49: Train Loss = 0.02278, Test Loss = 0.02340, Train accuracy score = 0.83082, Test accuracy score = 0.78302\nEpoch 50: Train Loss = 0.02309, Test Loss = 0.02371, Train accuracy score = 0.80587, Test accuracy score = 0.76101\nEpoch 51: Train Loss = 0.02297, Test Loss = 0.02373, Train accuracy score = 0.81038, Test accuracy score = 0.76478\nEpoch 52: Train Loss = 0.02291, Test Loss = 0.02377, Train accuracy score = 0.79780, Test accuracy score = 0.76132\nEpoch 53: Train Loss = 0.02304, Test Loss = 0.02360, Train accuracy score = 0.81426, Test accuracy score = 0.76447\nEpoch 54: Train Loss = 0.02286, Test Loss = 0.02397, Train accuracy score = 0.77180, Test accuracy score = 0.74277\nEpoch 55: Train Loss = 0.02294, Test Loss = 0.02348, Train accuracy score = 0.82694, Test accuracy score = 0.78302\nEpoch 56: Train Loss = 0.02274, Test Loss = 0.02339, Train accuracy score = 0.83375, Test accuracy score = 0.78459\nEpoch 57: Train Loss = 0.02279, Test Loss = 0.02344, Train accuracy score = 0.82170, Test accuracy score = 0.77767\nEpoch 58: Train Loss = 0.02295, Test Loss = 0.02334, Train accuracy score = 0.83281, Test accuracy score = 0.78679\nEpoch 59: Train Loss = 0.02274, Test Loss = 0.02334, Train accuracy score = 0.84235, Test accuracy score = 0.78994\nEpoch 60: Train Loss = 0.02274, Test Loss = 0.02354, Train accuracy score = 0.82306, Test accuracy score = 0.77170\nEpoch 61: Train Loss = 0.02321, Test Loss = 0.02338, Train accuracy score = 0.82998, Test accuracy score = 0.78302\nEpoch 62: Train Loss = 0.02325, Test Loss = 0.02346, Train accuracy score = 0.82704, Test accuracy score = 0.78270\nEpoch 63: Train Loss = 0.02265, Test Loss = 0.02334, Train accuracy score = 0.84748, Test accuracy score = 0.79245\nEpoch 64: Train Loss = 0.02264, Test Loss = 0.02328, Train accuracy score = 0.84004, Test accuracy score = 0.79245\nEpoch 65: Train Loss = 0.02252, Test Loss = 0.02319, Train accuracy score = 0.85115, Test accuracy score = 0.79654\nEpoch 66: Train Loss = 0.02249, Test Loss = 0.02346, Train accuracy score = 0.83417, Test accuracy score = 0.78428\nEpoch 67: Train Loss = 0.02248, Test Loss = 0.02327, Train accuracy score = 0.84665, Test accuracy score = 0.79214\nEpoch 68: Train Loss = 0.02257, Test Loss = 0.02348, Train accuracy score = 0.82558, Test accuracy score = 0.77673\nEpoch 69: Train Loss = 0.02249, Test Loss = 0.02321, Train accuracy score = 0.85262, Test accuracy score = 0.79119\nEpoch 70: Train Loss = 0.02257, Test Loss = 0.02343, Train accuracy score = 0.83270, Test accuracy score = 0.78994\nEpoch 71: Train Loss = 0.02242, Test Loss = 0.02309, Train accuracy score = 0.85325, Test accuracy score = 0.80346\nEpoch 72: Train Loss = 0.02258, Test Loss = 0.02334, Train accuracy score = 0.83679, Test accuracy score = 0.79057\nEpoch 73: Train Loss = 0.02254, Test Loss = 0.02327, Train accuracy score = 0.85409, Test accuracy score = 0.79465\nEpoch 74: Train Loss = 0.02242, Test Loss = 0.02320, Train accuracy score = 0.85377, Test accuracy score = 0.79717\nEpoch 75: Train Loss = 0.02265, Test Loss = 0.02333, Train accuracy score = 0.83732, Test accuracy score = 0.78459\nEpoch 76: Train Loss = 0.02251, Test Loss = 0.02333, Train accuracy score = 0.84371, Test accuracy score = 0.79025\nEpoch 77: Train Loss = 0.02238, Test Loss = 0.02308, Train accuracy score = 0.85734, Test accuracy score = 0.80660\nEpoch 78: Train Loss = 0.02247, Test Loss = 0.02325, Train accuracy score = 0.84937, Test accuracy score = 0.80189\nEpoch 79: Train Loss = 0.02242, Test Loss = 0.02320, Train accuracy score = 0.85231, Test accuracy score = 0.79403\nEpoch 80: Train Loss = 0.02249, Test Loss = 0.02306, Train accuracy score = 0.85755, Test accuracy score = 0.80629\nEpoch 81: Train Loss = 0.02234, Test Loss = 0.02313, Train accuracy score = 0.85786, Test accuracy score = 0.80189\nEpoch 82: Train Loss = 0.02233, Test Loss = 0.02337, Train accuracy score = 0.84130, Test accuracy score = 0.78585\nEpoch 83: Train Loss = 0.02229, Test Loss = 0.02323, Train accuracy score = 0.85252, Test accuracy score = 0.79245\nEpoch 84: Train Loss = 0.02237, Test Loss = 0.02326, Train accuracy score = 0.85839, Test accuracy score = 0.79811\nEpoch 85: Train Loss = 0.02236, Test Loss = 0.02326, Train accuracy score = 0.85346, Test accuracy score = 0.79686\nEpoch 86: Train Loss = 0.02251, Test Loss = 0.02325, Train accuracy score = 0.85231, Test accuracy score = 0.79528\nEpoch 87: Train Loss = 0.02232, Test Loss = 0.02310, Train accuracy score = 0.86017, Test accuracy score = 0.80220\nEpoch 88: Train Loss = 0.02242, Test Loss = 0.02328, Train accuracy score = 0.84853, Test accuracy score = 0.79906\nEpoch 89: Train Loss = 0.02248, Test Loss = 0.02327, Train accuracy score = 0.84843, Test accuracy score = 0.79308\nEpoch 90: Train Loss = 0.02244, Test Loss = 0.02322, Train accuracy score = 0.85451, Test accuracy score = 0.79780\nEpoch 91: Train Loss = 0.02228, Test Loss = 0.02322, Train accuracy score = 0.84633, Test accuracy score = 0.79811\nEpoch 92: Train Loss = 0.02240, Test Loss = 0.02317, Train accuracy score = 0.85461, Test accuracy score = 0.79748\nEpoch 93: Train Loss = 0.02233, Test Loss = 0.02343, Train accuracy score = 0.83344, Test accuracy score = 0.77925\nEpoch 94: Train Loss = 0.02225, Test Loss = 0.02324, Train accuracy score = 0.85797, Test accuracy score = 0.79780\nEpoch 95: Train Loss = 0.02233, Test Loss = 0.02309, Train accuracy score = 0.86719, Test accuracy score = 0.80723\nEpoch 96: Train Loss = 0.02227, Test Loss = 0.02326, Train accuracy score = 0.84507, Test accuracy score = 0.79434\nEpoch 97: Train Loss = 0.02230, Test Loss = 0.02296, Train accuracy score = 0.86813, Test accuracy score = 0.81164\nEpoch 98: Train Loss = 0.02217, Test Loss = 0.02309, Train accuracy score = 0.86803, Test accuracy score = 0.80252\nEpoch 99: Train Loss = 0.02226, Test Loss = 0.02355, Train accuracy score = 0.83187, Test accuracy score = 0.77327\nEpoch 100: Train Loss = 0.02235, Test Loss = 0.02318, Train accuracy score = 0.86237, Test accuracy score = 0.79560\nEpoch 101: Train Loss = 0.02227, Test Loss = 0.02328, Train accuracy score = 0.85818, Test accuracy score = 0.79025\nEpoch 102: Train Loss = 0.02246, Test Loss = 0.02336, Train accuracy score = 0.84885, Test accuracy score = 0.78711\nEpoch 103: Train Loss = 0.02241, Test Loss = 0.02308, Train accuracy score = 0.86771, Test accuracy score = 0.80818\nEpoch 104: Train Loss = 0.02231, Test Loss = 0.02313, Train accuracy score = 0.86237, Test accuracy score = 0.79811\nEpoch 105: Train Loss = 0.02230, Test Loss = 0.02327, Train accuracy score = 0.84570, Test accuracy score = 0.79025\nEpoch 106: Train Loss = 0.02231, Test Loss = 0.02315, Train accuracy score = 0.86625, Test accuracy score = 0.80440\nEpoch 107: Train Loss = 0.02220, Test Loss = 0.02315, Train accuracy score = 0.86499, Test accuracy score = 0.80063\nEpoch 108: Train Loss = 0.02277, Test Loss = 0.02346, Train accuracy score = 0.83449, Test accuracy score = 0.77704\nEpoch 109: Train Loss = 0.02246, Test Loss = 0.02328, Train accuracy score = 0.85713, Test accuracy score = 0.80000\nEpoch 110: Train Loss = 0.02236, Test Loss = 0.02319, Train accuracy score = 0.85839, Test accuracy score = 0.79057\nEpoch 111: Train Loss = 0.02234, Test Loss = 0.02328, Train accuracy score = 0.84392, Test accuracy score = 0.79403\nEpoch 112: Train Loss = 0.02227, Test Loss = 0.02318, Train accuracy score = 0.86310, Test accuracy score = 0.79340\nEpoch 113: Train Loss = 0.02228, Test Loss = 0.02317, Train accuracy score = 0.86006, Test accuracy score = 0.80126\nEpoch 114: Train Loss = 0.02229, Test Loss = 0.02321, Train accuracy score = 0.85472, Test accuracy score = 0.79560\nEpoch 115: Train Loss = 0.02228, Test Loss = 0.02300, Train accuracy score = 0.86845, Test accuracy score = 0.80597\nEpoch 116: Train Loss = 0.02233, Test Loss = 0.02315, Train accuracy score = 0.85901, Test accuracy score = 0.79403\nEpoch 117: Train Loss = 0.02245, Test Loss = 0.02363, Train accuracy score = 0.83176, Test accuracy score = 0.77296\nEpoch 118: Train Loss = 0.02237, Test Loss = 0.02317, Train accuracy score = 0.86363, Test accuracy score = 0.79748\nEpoch 119: Train Loss = 0.02229, Test Loss = 0.02329, Train accuracy score = 0.85451, Test accuracy score = 0.79528\nEpoch 120: Train Loss = 0.02230, Test Loss = 0.02316, Train accuracy score = 0.85409, Test accuracy score = 0.79906\nEpoch 121: Train Loss = 0.02240, Test Loss = 0.02315, Train accuracy score = 0.86153, Test accuracy score = 0.80157\nEpoch 122: Train Loss = 0.02216, Test Loss = 0.02303, Train accuracy score = 0.87264, Test accuracy score = 0.81384\nEpoch 123: Train Loss = 0.02288, Test Loss = 0.02336, Train accuracy score = 0.83941, Test accuracy score = 0.78553\nEpoch 124: Train Loss = 0.02255, Test Loss = 0.02344, Train accuracy score = 0.82358, Test accuracy score = 0.78616\nEpoch 125: Train Loss = 0.02258, Test Loss = 0.02318, Train accuracy score = 0.85493, Test accuracy score = 0.79780\nEpoch 126: Train Loss = 0.02241, Test Loss = 0.02320, Train accuracy score = 0.86509, Test accuracy score = 0.79843\nEpoch 127: Train Loss = 0.02222, Test Loss = 0.02315, Train accuracy score = 0.86719, Test accuracy score = 0.80220\nEpoch 128: Train Loss = 0.02213, Test Loss = 0.02318, Train accuracy score = 0.86184, Test accuracy score = 0.80189\nEpoch 129: Train Loss = 0.02219, Test Loss = 0.02299, Train accuracy score = 0.87285, Test accuracy score = 0.81289\nEpoch 130: Train Loss = 0.02233, Test Loss = 0.02339, Train accuracy score = 0.84696, Test accuracy score = 0.79057\nEpoch 131: Train Loss = 0.02236, Test Loss = 0.02312, Train accuracy score = 0.86468, Test accuracy score = 0.80566\nEpoch 132: Train Loss = 0.02210, Test Loss = 0.02311, Train accuracy score = 0.87442, Test accuracy score = 0.80314\nEpoch 133: Train Loss = 0.02204, Test Loss = 0.02298, Train accuracy score = 0.87715, Test accuracy score = 0.81101\nEpoch 134: Train Loss = 0.02205, Test Loss = 0.02286, Train accuracy score = 0.88333, Test accuracy score = 0.81541\nEpoch 135: Train Loss = 0.02208, Test Loss = 0.02307, Train accuracy score = 0.86604, Test accuracy score = 0.80629\nEpoch 136: Train Loss = 0.02226, Test Loss = 0.02305, Train accuracy score = 0.86845, Test accuracy score = 0.81226\nEpoch 137: Train Loss = 0.02200, Test Loss = 0.02311, Train accuracy score = 0.86929, Test accuracy score = 0.80503\nEpoch 138: Train Loss = 0.02205, Test Loss = 0.02335, Train accuracy score = 0.84507, Test accuracy score = 0.79088\nEpoch 139: Train Loss = 0.02226, Test Loss = 0.02319, Train accuracy score = 0.85849, Test accuracy score = 0.80346\nEpoch 140: Train Loss = 0.02271, Test Loss = 0.02422, Train accuracy score = 0.78029, Test accuracy score = 0.73050\nEpoch 141: Train Loss = 0.02266, Test Loss = 0.02338, Train accuracy score = 0.84759, Test accuracy score = 0.78962\nEpoch 142: Train Loss = 0.02242, Test Loss = 0.02327, Train accuracy score = 0.84832, Test accuracy score = 0.79245\nEpoch 143: Train Loss = 0.02252, Test Loss = 0.02328, Train accuracy score = 0.84549, Test accuracy score = 0.78868\nEpoch 144: Train Loss = 0.02230, Test Loss = 0.02321, Train accuracy score = 0.85744, Test accuracy score = 0.80157\nEpoch 145: Train Loss = 0.02234, Test Loss = 0.02345, Train accuracy score = 0.83260, Test accuracy score = 0.78365\nEpoch 146: Train Loss = 0.02254, Test Loss = 0.02314, Train accuracy score = 0.86268, Test accuracy score = 0.80126\nEpoch 147: Train Loss = 0.02212, Test Loss = 0.02310, Train accuracy score = 0.86992, Test accuracy score = 0.80409\nEpoch 148: Train Loss = 0.02209, Test Loss = 0.02301, Train accuracy score = 0.87956, Test accuracy score = 0.80881\nEpoch 149: Train Loss = 0.02200, Test Loss = 0.02307, Train accuracy score = 0.87117, Test accuracy score = 0.80660\nEpoch 150: Train Loss = 0.02212, Test Loss = 0.02310, Train accuracy score = 0.86268, Test accuracy score = 0.79843\n","output_type":"stream"}]},{"cell_type":"code","source":"pickle.dump(model, open('model_162.pkl', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-06-17T17:15:44.421182Z","iopub.execute_input":"2022-06-17T17:15:44.42188Z","iopub.status.idle":"2022-06-17T17:15:44.43652Z","shell.execute_reply.started":"2022-06-17T17:15:44.421837Z","shell.execute_reply":"2022-06-17T17:15:44.435788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import librosa\nimport pickle\n\nEmo = ['neutral','calm','happy','sad','angry','fear','disgust','surprise']\n\ndef extract_features(data,sample_rate):\n    # ZCR\n    result = np.array([])\n    zcr = np.mean(librosa.feature.zero_crossing_rate(y = data).T, axis = 0)\n    result=np.hstack((result, zcr))\n\n    # MFCC\n    mfcc = np.mean(librosa.feature.mfcc(y = data, sr = sample_rate).T, axis = 0)\n    result = np.hstack((result, mfcc))\n    \n    # Log Mel-Spectrogram\n    mel = np.mean(librosa.feature.melspectrogram(y = data, sr = sample_rate).T, axis = 0)\n    result = np.hstack((result, mel)) \n    \n    # Chroma\n    chroma_stft = np.mean(librosa.feature.chroma_stft(S = np.abs(librosa.stft(data)), sr = sample_rate).T, axis = 0)\n    result = np.hstack((result, chroma_stft))\n\n    # Root Mean Square Value\n    rms = np.mean(librosa.feature.rms(y = data).T, axis = 0)\n    result = np.hstack((result, rms))\n    \n    return result\n\ndef emotion_recognition(audio_file):\n    trained_model = pickle.load(open('model_162.pkl', 'rb'))\n    scaler = pickle.load(open('scaler.pkl','rb'))\n    \n    # load audio files with librosa\n    data, sample_rate = librosa.load(audio_file)\n    feat = extract_features(data,sample_rate)\n    feat = np.array(feat)\n    feat = feat[None,:]\n    sc_feat = scaler.transform(feat)\n    sc_feat = torch.from_numpy(sc_feat.astype('float32'))\n    prediction = trained_model(sc_feat.cuda())\n    pred = torch.argmax(prediction, dim=1)\n    return Emo[pred]","metadata":{"execution":{"iopub.status.busy":"2022-06-17T17:20:29.832946Z","iopub.execute_input":"2022-06-17T17:20:29.833306Z","iopub.status.idle":"2022-06-17T17:20:29.845645Z","shell.execute_reply.started":"2022-06-17T17:20:29.833274Z","shell.execute_reply":"2022-06-17T17:20:29.844719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio_file = \"/kaggle/input/ravdess-emotional-speech-audio/Actor_02/03-01-02-02-01-02-02.wav\"\nlabel = emotion_recognition(audio_file)\nprint(label)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T17:21:20.325758Z","iopub.execute_input":"2022-06-17T17:21:20.326112Z","iopub.status.idle":"2022-06-17T17:21:20.552327Z","shell.execute_reply.started":"2022-06-17T17:21:20.32608Z","shell.execute_reply":"2022-06-17T17:21:20.55151Z"},"trusted":true},"execution_count":null,"outputs":[]}]}